import { createContext, useState, useEffect, useCallback } from 'react';

export const AIContext = createContext();

export const AIProvider = ({ children }) => {
  const [messages, setMessages] = useState([]);
  const [conversationId, setConversationId] = useState(null);
  const [loading, setLoading] = useState(false);
  const [streamingMessageId, setStreamingMessageId] = useState(null);
  const [userScrolled, setUserScrolled] = useState(false);
  const [unreadMessages, setUnreadMessages] = useState(0);
  const [aiServiceStatus, setAiServiceStatus] = useState('unknown'); // ollama, openrouter, offline

  // Initialize conversation on mount
  useEffect(() => {
    initializeConversation();
    checkAIServiceStatus();
  }, []);

  const initializeConversation = useCallback(() => {
    try {
      const savedConversation = localStorage.getItem('hypo_conversation');
      if (savedConversation) {
        const { id, messages: savedMessages, timestamp } = JSON.parse(savedConversation);
        
        // Check if conversation is less than 24 hours old
        const isRecentConversation = Date.now() - timestamp < 24 * 60 * 60 * 1000;
        
        if (isRecentConversation && savedMessages?.length > 0) {
          setConversationId(id);
          setMessages(savedMessages.map(msg => ({
            ...msg,
            timestamp: new Date(msg.timestamp)
          })));
          console.log('âœ… Loaded saved conversation:', id);
          return;
        }
      }
      
      // Create new conversation
      const newId = `conv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      setConversationId(newId);
      setMessages([]);
      console.log('ðŸ†• Created new conversation:', newId);
      
    } catch (error) {
      console.error('âŒ Error initializing conversation:', error);
      // Fallback to new conversation
      const newId = `conv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      setConversationId(newId);
      setMessages([]);
    }
  }, []);

  // Check AI service status (Sonar, Ollama, OpenRouter)
  const checkAIServiceStatus = useCallback(async () => {
    try {
      const response = await fetch('/api/ai/hypo/health', {
        method: 'GET',
        headers: { 'Content-Type': 'application/json' }
      });
      
      const health = await response.json();
      
      if (health.ai?.service === 'sonar' && health.ai?.connection === 'connected') {
        setAiServiceStatus('sonar');
        console.log('ðŸŒ Sonar AI service detected and ready');
      } else if (health.ai?.service === 'ollama' && health.ai?.connection === 'connected') {
        setAiServiceStatus('ollama');
        console.log('ðŸ¦™ Ollama service detected and ready');
      } else if (health.ai?.service === 'openrouter') {
        setAiServiceStatus('openrouter');
        console.log('ðŸŒ OpenRouter service detected');
      } else {
        setAiServiceStatus('offline');
        console.log('âš ï¸ AI service unavailable');
      }
      
    } catch (error) {
      console.error('âŒ Error checking AI service status:', error);
      setAiServiceStatus('offline');
    }
  }, []);

  // Save conversation to localStorage whenever messages change
  useEffect(() => {
    if (conversationId && messages.length > 0) {
      try {
        const conversationData = {
          id: conversationId,
          messages: messages,
          timestamp: Date.now(),
          aiService: aiServiceStatus
        };
        localStorage.setItem('hypo_conversation', JSON.stringify(conversationData));
        console.log('ðŸ’¾ Saved conversation to localStorage');
      } catch (error) {
        console.error('âŒ Error saving conversation:', error);
      }
    }
  }, [messages, conversationId, aiServiceStatus]);

  const sendMessage = useCallback(async (message) => {
    if (!message?.trim() || loading || !conversationId) return;

    setLoading(true);
    setUserScrolled(false);
    setUnreadMessages(0);

    const userMsg = {
      id: Date.now(),
      sender: 'user',
      text: message.trim(),
      timestamp: new Date()
    };

    const aiMsg = {
      id: Date.now() + 1,
      sender: 'ai',
      text: '',
      timestamp: new Date(),
      streaming: true
    };

    setMessages(prev => [...prev, userMsg, aiMsg]);
    setStreamingMessageId(aiMsg.id);

    try {
      const response = await fetch('/api/ai/hypo/chat-stream', {
        method: 'POST',
        headers: { 
          'Content-Type': 'application/json',
          'Accept': 'text/event-stream'
        },
        body: JSON.stringify({ 
          message: message.trim(),
          conversationId: conversationId
        })
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const reader = response.body.getReader();
      const decoder = new TextDecoder();

      while (true) {
        const { value, done } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            try {
              const data = JSON.parse(line.slice(6));
              
              if (data.type === 'chunk') {
                setMessages(prev => prev.map(msg => 
                  msg.id === aiMsg.id 
                    ? { ...msg, text: msg.text + data.content }
                    : msg
                ));
              } else if (data.type === 'done') {
                setMessages(prev => prev.map(msg => 
                  msg.id === aiMsg.id 
                    ? { 
                        ...msg, 
                        streaming: false,
                        modelUsed: data.modelUsed,
                        processingTime: data.processingTime
                      }
                    : msg
                ));
                setStreamingMessageId(null);
                
                // Update AI service status if provided
                if (data.service) {
                  setAiServiceStatus(data.service);
                } else if (data.modelUsed) {
                  // Fallback logic for older responses
                  if (data.modelUsed.includes('sonar') || data.service === 'sonar') {
                    setAiServiceStatus('sonar');
                  } else if (data.modelUsed.includes('llama')) {
                    setAiServiceStatus('ollama');
                  } else {
                    setAiServiceStatus('openrouter');
                  }
                }
                
              } else if (data.type === 'error') {
                let errorMessage = data.message || 'CÃ³ lá»—i xáº£y ra. Vui lÃ²ng thá»­ láº¡i.';
                
                // Enhanced error handling for multiple services
                if (data.errorType === 'sonar_api_error') {
                  errorMessage = 'ðŸŒ Sonar API error. Kiá»ƒm tra API key hoáº·c thá»­ láº¡i sau.';
                  setAiServiceStatus('offline');
                  
                } else if (data.errorType === 'sonar_rate_limit') {
                  errorMessage = 'â³ Sonar rate limit exceeded. Vui lÃ²ng Ä‘á»£i má»™t chÃºt.';
                  
                } else if (data.errorType === 'ollama_not_running') {
                  errorMessage = 'ðŸ¦™ Ollama service chÆ°a cháº¡y. Vui lÃ²ng start Ollama: ollama serve';
                  setAiServiceStatus('offline');
                  
                } else if (data.errorType === 'model_not_found') {
                  errorMessage = 'ðŸ” Model khÃ´ng tá»“n táº¡i. Vui lÃ²ng pull model: ollama pull llama3.2:3b';
                  setAiServiceStatus('offline');
                  
                } else if (data.errorType === 'connection_failed') {
                  errorMessage = 'ðŸ”Œ KhÃ´ng thá»ƒ káº¿t ná»‘i AI service. Kiá»ƒm tra cáº¥u hÃ¬nh.';
                  setAiServiceStatus('offline');
                  
                } else if (data.errorType === 'out_of_memory') {
                  errorMessage = 'ðŸ’¾ KhÃ´ng Ä‘á»§ RAM Ä‘á»ƒ cháº¡y model. Thá»­ model nhá» hÆ¡n: ollama pull llama3.2:1b';
                  
                } else if (data.errorType === 'timeout') {
                  errorMessage = 'â° AI response timeout. Model cÃ³ thá»ƒ Ä‘ang táº£i hoáº·c máº¡ng cháº­m.';
                  
                } else if (data.errorType === 'model_not_loaded') {
                  errorMessage = 'ðŸ“¦ Model chÆ°a Ä‘Æ°á»£c load. Äang táº£i model, vui lÃ²ng thá»­ láº¡i sau 30-60 giÃ¢y...';
                  
                } else if (data.errorType === 'quota_exceeded') {
                  errorMessage = 'ðŸš« OpenRouter Ä‘Ã£ Ä‘áº¡t giá»›i háº¡n requests hÃ´m nay. Sáº½ reset vÃ o 7:00 AM mai.';
                  setAiServiceStatus('openrouter');
                  
                } else if (data.errorType === 'rate_limited') {
                  errorMessage = 'â³ QuÃ¡ nhiá»u requests. Vui lÃ²ng Ä‘á»£i 30 giÃ¢y trÆ°á»›c khi thá»­ láº¡i.';
                  
                } else if (data.errorType === 'provider_rate_limited') {
                  errorMessage = 'ðŸš« Google Gemini Ä‘ang quÃ¡ táº£i. Vui lÃ²ng thá»­ láº¡i sau 1-2 phÃºt hoáº·c Ä‘á»•i sang Ollama.';
                  setAiServiceStatus('openrouter');
                }
                
                setMessages(prev => prev.map(msg => 
                  msg.id === aiMsg.id 
                    ? { 
                        ...msg, 
                        text: errorMessage, 
                        streaming: false, 
                        isError: true,
                        errorType: data.errorType,
                        suggestion: data.suggestion
                      }
                    : msg
                ));
                setStreamingMessageId(null);
              }
            } catch (parseError) {
              console.error('âŒ Parse error:', parseError);
            }
          }
        }
      }

    } catch (error) {
      console.error('âŒ Send message error:', error);
      
      let errorMessage = 'CÃ³ lá»—i xáº£y ra. Vui lÃ²ng thá»­ láº¡i.';
      let errorType = 'network_error';
      
      if (error.message.includes('429')) {
        errorMessage = 'â³ QuÃ¡ nhiá»u yÃªu cáº§u. Vui lÃ²ng Ä‘á»£i má»™t chÃºt.';
        errorType = 'rate_limited';
      } else if (error.message.includes('500')) {
        errorMessage = 'ðŸ”§ Server táº¡m thá»i gáº·p sá»± cá»‘. Vui lÃ²ng thá»­ láº¡i.';
        errorType = 'server_error';
      } else if (error.message.includes('503')) {
        errorMessage = 'ðŸ¦™ AI service khÃ´ng kháº£ dá»¥ng. Kiá»ƒm tra Ollama hoáº·c thá»­ OpenRouter.';
        errorType = 'service_unavailable';
        setAiServiceStatus('offline');
      } else if (error.name === 'TypeError' && error.message.includes('fetch')) {
        errorMessage = 'ðŸŒ Lá»—i káº¿t ná»‘i máº¡ng. Kiá»ƒm tra internet vÃ  thá»­ láº¡i.';
        errorType = 'network_error';
      }
      
      setMessages(prev => prev.map(msg => 
        msg.id === aiMsg.id 
          ? { 
              ...msg, 
              text: errorMessage, 
              streaming: false, 
              isError: true,
              errorType: errorType
            }
          : msg
      ));
      setStreamingMessageId(null);
    } finally {
      setLoading(false);
    }
  }, [loading, conversationId]);

  const clearConversation = useCallback(async () => {
    try {
      // Clear from server if conversation exists
      if (conversationId && messages.length > 0) {
        await fetch(`/api/ai/hypo/conversation/${conversationId}`, {
          method: 'DELETE',
          headers: { 'Content-Type': 'application/json' }
        });
      }
    } catch (error) {
      console.warn('âš ï¸ Failed to clear conversation from server:', error);
    } finally {
      // Always clear locally
      localStorage.removeItem('hypo_conversation');
      setMessages([]);
      const newId = `conv_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
      setConversationId(newId);
      setStreamingMessageId(null);
      setUserScrolled(false);
      setUnreadMessages(0);
      console.log('ðŸ—‘ï¸ Cleared conversation, created new:', newId);
    }
  }, [conversationId, messages.length]);

  const retryLastMessage = useCallback(async () => {
    if (messages.length < 2) return;
    
    // Find the last user message
    const lastUserMessage = [...messages].reverse().find(msg => msg.sender === 'user');
    if (!lastUserMessage) return;
    
    // Remove the last AI response (if any) and retry
    setMessages(prev => {
      const lastAiIndex = prev.map(m => m.sender).lastIndexOf('ai');
      if (lastAiIndex !== -1) {
        return prev.slice(0, lastAiIndex);
      }
      return prev;
    });
    
    // Resend the last user message
    await sendMessage(lastUserMessage.text);
  }, [messages, sendMessage]);

  const switchAIService = useCallback(async () => {
    try {
      // This could be implemented to switch between Ollama and OpenRouter
      // For now, just refresh the status
      await checkAIServiceStatus();
    } catch (error) {
      console.error('âŒ Error switching AI service:', error);
    }
  }, [checkAIServiceStatus]);

  const getConversationStats = useCallback(() => {
    const userMessages = messages.filter(m => m.sender === 'user').length;
    const aiMessages = messages.filter(m => m.sender === 'ai' && !m.isError).length;
    const errorMessages = messages.filter(m => m.isError).length;
    
    return {
      totalMessages: messages.length,
      userMessages,
      aiMessages,
      errorMessages,
      conversationId,
      aiService: aiServiceStatus,
      hasConversation: messages.length > 0
    };
  }, [messages, conversationId, aiServiceStatus]);

  const value = {
    // Core state
    messages,
    conversationId,
    loading,
    streamingMessageId,
    aiServiceStatus,
    
    // Scroll and UI state
    userScrolled,
    setUserScrolled,
    unreadMessages,
    setUnreadMessages,
    
    // Core actions
    sendMessage,
    clearConversation,
    retryLastMessage,
    
    // Service management
    checkAIServiceStatus,
    switchAIService,
    
    // Utilities
    getConversationStats,
    
    // Computed values
    hasMessages: messages.length > 0,
    canSendMessage: !loading && conversationId,
    isSonarReady: aiServiceStatus === 'sonar',
    isOllamaReady: aiServiceStatus === 'ollama',
    isOffline: aiServiceStatus === 'offline'
  };

  return (
    <AIContext.Provider value={value}>
      {children}
    </AIContext.Provider>
  );
};

// Additional context for AI service status
export const AIStatusContext = createContext();

export const AIStatusProvider = ({ children }) => {
  const [serviceHealth, setServiceHealth] = useState({
    sonar: false,
    ollama: false,
    openrouter: false,
    lastCheck: null
  });

  const checkAllServices = useCallback(async () => {
    try {
      const response = await fetch('/api/ai/hypo/health');
      const health = await response.json();
      
      setServiceHealth({
        perplexity: health.ai?.service === 'perplexity' && health.ai?.connection === 'connected',
        ollama: health.ai?.service === 'ollama' && health.ai?.connection === 'connected',
        openrouter: health.ai?.service === 'openrouter' && health.status === 'healthy',
        lastCheck: new Date()
      });
      
    } catch (error) {
      console.error('âŒ Error checking service health:', error);
      setServiceHealth(prev => ({
        ...prev,
        perplexity: false,
        ollama: false,
        openrouter: false,
        lastCheck: new Date()
      }));
    }
  }, []);

  useEffect(() => {
    checkAllServices();
    
    // Check every 30 seconds
    const interval = setInterval(checkAllServices, 30000);
    return () => clearInterval(interval);
  }, [checkAllServices]);

  return (
    <AIStatusContext.Provider value={{ serviceHealth, checkAllServices }}>
      {children}
    </AIStatusContext.Provider>
  );
};
