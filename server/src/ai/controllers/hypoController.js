const { generateOpenRouterChatStream } = require('../services/openrouter-service');

// Simple in-memory rate limiting
const lastRequestTime = new Map();
const REQUEST_COOLDOWN = 3000; // 3 seconds between requests per user

// Original chat endpoint (keep for compatibility)
exports.chat = async (req, res) => {
  console.log('=== ğŸ¤– AI CHAT REQUEST START ===');
  
  try {
    const { message } = req.body;
    
    if (!message) {
      console.log('âŒ No message in request body');
      return res.status(400).json({ error: 'Message is required' });
    }
    
    console.log('âœ… Received message:', message);
    
    const messages = [
      {
        role: "system",
        content: "Báº¡n lÃ  Hypo, AI Assistant cá»§a team HYTEAM. Báº¡n thÃ¢n thiá»‡n, há»¯u Ã­ch vÃ  chuyÃªn vá» quáº£n lÃ½ dá»± Ã¡n. HÃ£y tráº£ lá»i ngáº¯n gá»n vÃ  há»¯u Ã­ch."
      },
      {
        role: "user", 
        content: message
      }
    ];
    
    console.log('ğŸ“¤ Calling OpenRouter API...');
    const aiReply = await generateOpenRouterChat(messages);
    console.log('ğŸ“¥ AI Reply:', aiReply);
    
    const response = { message: aiReply };
    res.json(response);
    
  } catch (err) {
    console.error('âŒ AI ERROR:', err);
    res.status(500).json({ 
      error: 'AI service error', 
      detail: err.message
    });
  }
  
  console.log('=== ğŸ¤– AI CHAT REQUEST END ===');
};

// Enhanced streaming chat endpoint vá»›i comprehensive error handling
exports.chatStream = async (req, res) => {
  try {
    const { message } = req.body;
    const userKey = req.ip || 'default';
    
    // Check rate limiting
    const now = Date.now();
    const lastTime = lastRequestTime.get(userKey) || 0;
    
    if (now - lastTime < REQUEST_COOLDOWN) {
      const waitTime = Math.ceil((REQUEST_COOLDOWN - (now - lastTime)) / 1000);
      return res.status(429).json({ 
        error: 'Too many requests', 
        message: `Please wait ${waitTime} seconds before sending another message.`,
        waitTime: waitTime
      });
    }
    
    lastRequestTime.set(userKey, now);
    
    if (!message) {
      return res.status(400).json({ error: 'Message is required' });
    }
    
    console.log('=== ğŸŒŠ AI STREAMING REQUEST START ===');
    console.log('âœ… Received streaming message:', message);
    
    // Set headers for Server-Sent Events
    res.writeHead(200, {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'Access-Control-Allow-Origin': '*',
    });

    const messages = [
      {
        role: "system",
        content: "Báº¡n lÃ  Hypo, AI Assistant cá»§a team HYTEAM. Báº¡n thÃ¢n thiá»‡n, há»¯u Ã­ch vÃ  chuyÃªn vá» quáº£n lÃ½ dá»± Ã¡n. HÃ£y tráº£ lá»i ngáº¯n gá»n vÃ  há»¯u Ã­ch."
      },
      {
        role: "user", 
        content: message
      }
    ];

    let responseText = '';
    
    try {
      console.log('ğŸ“¤ Starting streaming response...');
      
      await generateOpenRouterChatStream(messages, (chunk) => {
        responseText += chunk;
        res.write(`data: ${JSON.stringify({ type: 'chunk', content: chunk })}\n\n`);
      });

      console.log('âœ… Streaming completed');
      res.write(`data: ${JSON.stringify({ type: 'done', fullText: responseText })}\n\n`);
      res.end();

    } catch (streamError) {
      console.error('âŒ STREAMING ERROR:', streamError);
      
      // Enhanced error categorization
      let errorType = 'general_error';
      let errorMessage = 'CÃ³ lá»—i xáº£y ra. Vui lÃ²ng thá»­ láº¡i.';
      
      // Check for specific quota errors
      if (streamError.message?.includes('free-models-per-day') || 
          streamError.message?.includes('Daily quota exceeded') ||
          streamError.message?.includes('quota_exceeded') ||
          streamError.message?.includes('rate limit exceeded for free tier')) {
        errorType = 'quota_exceeded';
        errorMessage = 'ğŸš« AI Ä‘Ã£ Ä‘áº¡t giá»›i háº¡n requests hÃ´m nay. Sáº½ reset vÃ o 7:00 AM mai. Vui lÃ²ng thá»­ láº¡i sau!';
        
      } else if (streamError.message?.includes('rate_limit') ||
                 streamError.message?.includes('too_many_requests') ||
                 streamError.response?.status === 429) {
        errorType = 'rate_limited';
        errorMessage = 'â³ QuÃ¡ nhiá»u requests. Vui lÃ²ng Ä‘á»£i 30 giÃ¢y trÆ°á»›c khi thá»­ láº¡i.';
        
      } else if (streamError.message?.includes('model_unavailable') ||
                 streamError.message?.includes('service_unavailable')) {
        errorType = 'model_unavailable';
        errorMessage = 'ğŸ”§ AI model táº¡m thá»i khÃ´ng kháº£ dá»¥ng. Vui lÃ²ng thá»­ láº¡i sau.';
        
      } else if (streamError.message?.includes('context_length_exceeded')) {
        errorType = 'context_too_long';
        errorMessage = 'ğŸ“ Cuá»™c trÃ² chuyá»‡n quÃ¡ dÃ i. Vui lÃ²ng báº¯t Ä‘áº§u chat má»›i.';
      }
      
      console.log(`ğŸ·ï¸ Error categorized as: ${errorType}`);
      console.log(`ğŸ’¬ Error message: ${errorMessage}`);
      
      // Send structured error response
      res.write(`data: ${JSON.stringify({ 
        type: 'error', 
        errorType: errorType,
        message: errorMessage,
        originalError: streamError.message
      })}\n\n`);
      res.end();
    }

  } catch (err) {
    console.error('âŒ CONTROLLER ERROR:', err);
    
    // Last resort error handling
    if (!res.headersSent) {
      if (err.response?.status === 429 || err.message?.includes('quota exceeded')) {
        res.write(`data: ${JSON.stringify({ 
          type: 'error', 
          errorType: 'quota_exceeded',
          message: 'ğŸš« AI Ä‘Ã£ Ä‘áº¡t giá»›i háº¡n requests hÃ´m nay. Sáº½ reset vÃ o 7:00 AM mai. Vui lÃ²ng thá»­ láº¡i sau!' 
        })}\n\n`);
      } else {
        res.write(`data: ${JSON.stringify({ 
          type: 'error', 
          errorType: 'general_error',
          message: 'CÃ³ lá»—i xáº£y ra. Vui lÃ²ng thá»­ láº¡i.' 
        })}\n\n`);
      }
      res.end();
    }
  }
  
  console.log('=== ğŸŒŠ AI STREAMING REQUEST END ===');
};
